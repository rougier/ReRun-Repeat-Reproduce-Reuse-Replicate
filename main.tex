% !TeX program = pdflatex
\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}

% Wider margins (compared to default)
\usepackage[margin=2.5cm]{geometry}

% English support (typography and hyphenation)
\usepackage[american]{babel}
\usepackage{csquotes}

% Unicode encoding
\usepackage[utf8]{inputenc}

% Better default font (Libertine and Inconsolata)
\usepackage[ttscale=.875]{libertine}
\usepackage[scaled=0.96]{zi4}

%\usepackage{mathspec}
%\setmainfont{Minion Pro Cond}
%\setmathsfont(Digits,Greek,Latin)[Numbers={Proportional}]{Minion Pro}
%\setmathrm{Minion Pro}
%\setsansfont{MyriadPro-Cond}


% Biber
\usepackage[backend=biber,
            style=apa,
            maxcitenames=3,
            maxbibnames=99,
            apamaxprtauth=99,	
            natbib=true]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\bibliography{library.bib}

% Graphics
\usepackage{graphicx}


% Code Listing
\usepackage{listings}
\usepackage[framemethod=tikz]{mdframed}
\makeatletter
\def\mdf@@codeheading{Code Listings}
\define@key{mdf}{title}{\def\mdf@@codeheading{#1}}
\mdfdefinestyle{lstlisting}{%
  backgroundcolor=black!2.5,
  innertopmargin=2pt,
  middlelinewidth=0.75pt,
  outerlinewidth=9pt,
  outerlinecolor=white,
  innerleftmargin=10pt,
  innerrightmargin=10pt,
  leftmargin=0pt,
  rightmargin=0pt,
  rightline=false,
  leftline=false,
  bottomline=false,
  skipabove=\topskip,
  skipbelow=\topskip,
  roundcorner=0pt,
  singleextra={
    \node[text=black, % fill=white, draw,
          anchor=south west, yshift=-0.25pt, xshift=5pt,
          font=\footnotesize] at (O|-P) {\csname mdf@@codeheading\endcsname};},
  firstextra={
    \node[text=black, % fill=white, draw,
          anchor=south west, yshift=-0.5pt, xshift=5pt,
          font=\footnotesize] at (O|-P) {\csname mdf@@codeheading\endcsname};}
}

\lstset{ %
  basicstyle=\small\tt\linespread{0.75},
  language=Python,
  commentstyle=\color{gray},
  columns=fullflexible,
}

\lstnewenvironment{code}[2][]{%
  \lstset{#1}%
  \mdframed[style=lstlisting,title={#2}]%
}{\endmdframed}


%\usepackage{listings}
%\surroundwithmdframed[
%%  hidealllines=true,
%  linewidth=0.25pt,
%  linecolor=black!50,
%  backgroundcolor=black!4,
%  innerleftmargin=8pt,
%  innertopmargin=3pt,
%  innerbottommargin=3pt]{lstlisting}
%\lstset{ %
%  basicstyle=\small\tt\linespread{0.75},
%  language=Python,
%  commentstyle=\color{gray},
%  columns=fullflexible,
%}


% Hyperref
\usepackage{xcolor}
\definecolor{blendedblue}{rgb}{0.2, 0.2, 0.6}
\definecolor{blendedred}{rgb}{0.8, 0.2, 0.2}
\usepackage[bookmarks=true,
            breaklinks=true,
            pdfborder={0 0 0},
            citecolor=blendedblue,
            colorlinks=true,
            linkcolor=blendedblue,
            urlcolor=blendedblue,
            citecolor=blendedblue,
            linktocpage=false,
            hyperindex=true,
            linkbordercolor=white]{hyperref}
\usepackage{hyperref}
\hypersetup{colorlinks=true}


\title{Re-run, Repeat, Reproduce, Re-use, Replicate:\\Transforming Code into Scientific Contributions}
% \title{Run Python, run!}
% \title{The R Quintuplet (R$^5$)}


\author{Nicolas P. Rougier and Fabien C. Y. Benureau}
\date{\today}

\begin{document}
\maketitle
% -------------------------
\section*{Introduction (R$^{\mathbf 0}$)}

Replicability is a cornerstone of science.
If an experimental result cannot be re-obtained by an independent party,
% it might be because of a fluke, a bias, a bug, a false positive or even a hack.
%it becomes, at best, nothing more than food for thought for future research. 
it merely becomes, at best, an observation that
may feed future research (\cite{Mesirov:2010,osc:2015}). 
Replication issues have received increased attention in recent years,
in the domain of medicine and psychology in particular (\cite{Iqbal:2016}).

One could think that computer science would mostly be shielded from such issues.
But precisely because it is easy to believe that
if a program runs once and gives expected results it will do so forever, 
crucial steps to transform working code into meaningful scientific contributions are rarely undertaken \citep{Sandve:2013,Schwab:2000}. 
Computer science is plagued by replication problems,
in part, precisely because it seems impervious to them.

In fact, a program can fail as a scientific contribution
in many different ways for many different reasons.
Borrowing the terms coined by \citeauthor{Goble:2016} \citep{Goble:2016},
for your program to contribute to Science,
you have to make sure it is
re-runnable (R$^1$),
repeatable (R$^2$),
reproducible (R$^3$),
re-usable (R$^4$)
and replicable (R$^5$).

Let us illustrate this with a very simple example,
a random walk written in Python:

\begin{code}{\textbf{\textsc{Listing 1:}} Random walk (R$^0$)}
import random

x = 0
for i in xrange(10):
    step = random.choice([-1,+1])
    x += step
    print x,
\end{code}
% That would be too easy if the program was commented...
% # randomly choose ten instances of either 1 or -1. 
% # compute the cumulative sum (the sum of all preceeding elements)

Executed, this program would display 
\begin{code}{Output}
-1  0 -1 -2 -1 -2 -3 -2 -1 -2 # with the steps being -1, +1, -1, -1, +1, -1, -1, +1, +1, -1
\end{code}

What could go wrong with such a simple program?\\
\vfill
Well...
\vfill


% -------------------------
\clearpage
\section*{Re-runnable (R$^{\mathbf 1}$)}

Have you ever try to re-run a program you wrote some years ago?
Frustratingly, it can often be surprisingly hard. 
Part of the problem is that technology is evolving at a fast pace
and you cannot  know in advance
how the system, the software and the libraries your program depends on will evolve.
Since you wrote the code,
you may have reinstalled or upgraded  your operating system.
The compiler, interpreter or set of libraries installed may have been replaced with newer versions. 
You may find yourself battling with
arcane issues of library compatibility---thoroughly orthogonal to your immediate research  goals---to execute again a code \emph{that worked perfectly before}. 
% "thoroughly orthogonal to your immediate research goals": possibly problematic

To be clear, it is impossible to write future-proof code,
and the best efforts can be stymied by the smallest change in one of the dependencies.
At the same time, modernizing an unmaintained ten-year-old code
can reveal itself to be an arduous and expensive undertaking,
with each change risking to affect the semantics of the program.

Rather trying to predict the future or painstakingly dusting off old code, 
an often easier and less dangerous solution is to recreate the old execution environment.
For this to happen however, 
the dependencies in terms of systems, software and libraries must be made clear enough.

A \emph{re-runnable} code is one that describes an execution environment in which it is executable with enough details to be recreated.
As shown by \citep{Collberg:2016}, this is far from being either obvious or easy.

% * <fabien.benureau+overleaf@gmail.com> 2017-05-08T18:09:47.037Z:
% 
% Wouldn't Collberg be better cited in R^2?
% 
% ^.
% if using random.choices, won't be runnable with anything <3.6
% if using itertools.accumulate, won't be runnable with anything <3.2
% although perhaps not the best syntax, we might want to keep the
% import mod
% mod.function 
% syntax for both random and intertools
%
% Problem with current code is that it is highly inefficient now (compared to R0)
\begin{code}{\textbf{\textsc{Listing 2:}} Random walk (R$^1$)}
# Tested with Python 3
import random

walk, total = [], 0
for i in range(10):
    step = random.choice([-1,+1])     
    total += step
    walk.append(total)
    
print(walk)
\end{code}

In our case, the R$^0$ version of our tiny walker seems to imply
that any version of Python would be fine.
This not the case because it uses the print {\em instruction} and the {\tt xrange} operator, both specific to Python 2.
The print {\em instruction}, available in Python 2 (a version still widely used; support is scheduled to stop in 2020), 
has been deprecated in Python 3 (first released in 2008, almost a decade ago)
in favor or a  print {\em function}.
The {\tt xrange} operator has been replaced by the {\tt range} operator in Python 3.
In order to try to future-proof the code a bit, we might as well target Python 3, as is done in the R$^1$ version.
Incidentally, it remains compatible with Python 2.

But whatever version is chosen,
the crucial step here is to document it.


% \clearpage
\section*{Repeatable (R$^{\mathbf 2}$)}

The code is running and producing the expected results. 
The next step is to make sure that you can produce the same output over successive runs of your program. 
In other words, the next step is to make your program deterministic,  producing {\em repeatable} output. 
Such repeatability is useful. 
If a run of the program produces a particularly puzzling results,  it allows you to scrutinize any step of the execution of the program by re-running it again with extraneous prints, or inside a debugger. 
It is also the easiest way to prove that the program did indeed produce the published results.
Such repeatability is not always possible or easy \citep{Diethelm:2012, Courtes:2015}.
But for sequential programs not depending on exotic hardware, it often comes down to controlling the initialization of the pseudo-random number generators (RNG).\\ 

For our program, that means setting the seed of the {\tt random} module. We may also want to save the output of the program to a file, so that we can easily verify that consecutive runs do produce the same output: eyeballing differences is unreliable and  time-consuming---and therefore won't be done systematically.

\begin{code}{\textbf{\textsc{Listing 3:}} Random walk (R$^2$)}
# Tested with Python 3
import random

random.seed(0) # RNG initialization

walk, total = [], 0
for i in range(10):
    step = random.choice([-1,+1])     
    total += step
    walk.append(total)
    
print(walk)
with open("results.txt", "w") as file: # Saving output to disk
    file.write(str(walk))
\end{code}

Setting seeds is not without danger. 
Using 439 as a seed in the previous program would result in ten consecutive +1 steps, which---although a perfectly valid random walk---lend itself to a gross misinterpretation of the overall dynamic of the algorithm. 
Verifying that the qualitative aspects of the results and the conclusion we make are not tied to a specific initialization of the pseudo-random generator is an integral part of any scientific undertaking in computational science; 
this is usually done by repeating the simulations multiple times with different seeds.  

%Determinism is a tool that allow to study bugs and spurious or unexpected behavior. 
%Determinism is also critical for other to reproduce and study your results. 
% talk about controversy about setting seeds

% For example, you can play with some parameters and
% find a specific set that produce interesting results that you save to a file.
% In the meantime, you continue playing with parameters in order get new results.
% But what if you want to come back to this one specific result you just saved?
% Did you save as well the set of parameters tied to this specific results? 
% This kind of situation happens quite regularly in the scientific literature \cite{Claerbout:2000} 
% such that in some case, it is virtually impossible
% to find the set of parameters that has been used to produce this or that figure.
% If you're note careful enough, this will happen to you as well.\\


%\clearpage
\section*{Reproducible (R$^{\mathbf 3}$)}

% Reproducible definition + R^2 not reproducible
The R$^2$ code seems fine enough, but it hides several problems that come to light when trying to {\em reproduce} results.
A result is said to be \emph{reproducible} if another researcher can take the original code and input data, execute it, and re-obtain the result \parencite{Peng:2006}. As explained by \citeauthor{Donoho:2009} \parencite{Donoho:2009}, scientific practice must expect that {\em errors are ubiquitous}, and therefore be robust to them. Ensuring reproducibility is an important step toward this: it allows other to verify that your code does indeed produce the published results, and scrutinize the way that it does.\\

As explained by \citeauthor{Mesnard:2016} \citep{Mesnard:2016}, reproducibility is hard. 
For instance, the R$^2$ program will not produce the same results all the time. It will, because it is repeatable, produce the same results over repeated executions.
But it will not do so over different execution environments. The cause is found in a change that occurred in the pseudo-random number generator between Python 3.2 and Python 3.3. 
Executed with Python 2.7 to 3.2, the code will produce the sequence 1, 2, 1, 2, 3, 4, 5, 6, 7, 6. But with Python 3.3 to 3.6, it will produce 1, 2, 1, 0, 1, 0, 1, 0, -1, 0.\\

% Reproducible definition + R^2 not reproducible
Because any dependency of your program---to the most basic one, the language itself---can change its behavior from one version to another, executability (R$^1$) and determinism (R$^2$) are necessary but not sufficient for reproducibility. The exact execution environment used to produce the results must also be specified---rather than the broadest set of environments where the code can be executed. In other words, assertions such as "the results were obtained with Python 3.6.1" are more valuable, in a scientific context, than "the program should work with Python 3.3.x and above".\\

% Reproducibility is hard
Because of the increasing complexity of our computational stacks, retrieving and deciding what is pertinent (CPU architecture? Operating system version? Endianness?) might be difficult.
See Appendix \ref{appendix:executionenvs} for a discussion.\\

% Reproducibility in practice.
Having precisely described the execution environment, reproducibility is technically obtained. However, \emph{reproducibility in practice} requires more than that.

% Saving Parameters
The R$^2$ program uses a random seed but does not keep a trace of it except in the code. Therefore, should the code change after the production of the results, someone provided with the result file will not be able to know which seed was used, and would need to iterate through all possible random seeds, which is not possible in practice. R$^3$ saves the seed in the name of the result file. More generally, results should always come alongside their context, i.e. an exhaustive list of the parameters used and a description of the execution environment.\\

% Data behind graphs
Graphs in articles are powerful way to communicate complex information. They are imprecise and cumbersome, and sometimes plain unusable, however when it comes to checking that the reproduced data match the one published in the article. A good figure should always be accompanied by its underlying data to allow straightforward comparisons.\\

% Unit test for R^3
Another good practice is to make the code self-verifiable. In R$^3$, a short unit test is provided, that allows the code to verify its own reproducibility. Should this test fail, then there is little hope of reproducing the results. Of course, passing the test is no definitive proof.\\

% Reproducibility implies availability 
It should be obvious by now but \emph{reproducibility implies availability}. As shown in \citep{Collberg:2016}, code is often unavailable, or only available upon request. While the latter may seem sufficient, changes in email address, changes in career, retirement, a busy inbox or poor archiving practices can make a code just as unreachable. Code \emph{and input and result files} should be available with the published article, as supplementary data, or through a DOI link to a scientific repository such as Figshare or Zenodo\footnote{Online code repository such as GitHub \emph{are not} scientific repositories, and may disappear, change name, etc. at any moment. Direct links to them are not perpetual and should be avoided.}.\\

%\begin{itemize}
%\itemsep 0em
%\item Python 3
%\item Python 3.6
%\item {Python 3.6.1 (default, Mar 28 2017, 10:33:50) \\
%      {\tt [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)] on darwin}}
%\item {Python 3.6.1 (default, Mar 28 2017, 10:33:50) \\
%      {\tt [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)] on darwin}\\
%      {\tt configuration variables: -std=c99 -Wextra -Wno-unused-result -Wno-unused-parameter -Wno-missing-field-initializers ...}}
%\end{itemize}

% Adding license for diffusion
% Lorena Barba example (Science blog)
 
\begin{code}{\textbf{\textsc{Listing 4:}} Random walk (R$^3$)}
# Copyright (c) 2017 Nicolas P. Rougier and Fabien C. Y. Benureau
# Release under the BSD 2-clause license
# Tested with CPython 3.6 / macOS 10.12.4 / 64 bits architecture
import random
from itertools import accumulate

def walk(n):
    """ Random walk for n steps """
  
    steps = [-1 if random.uniform(-1,+1) < 0 else +1 for i in range(n)]
    return list(accumulate(steps))

if __name__ == '__main__':
    # Unit test
    random.seed(1)
    assert walk(10) == [-1, 0, 1, 0, -1, -2, -1, 0, -1, -2]

    # Random walk for 10 steps
    seed = 1
    random.seed(seed)
    x = walk(10)
    
    # Display & save results
    print(x)
    with open("results-R3-%d.txt" % seed, "w") as file:
        file.write(str(x))
\end{code}

% Summary
To recap, reproducibility implies re-runnability or repeatability and availability, yet  imposes additional conditions. Dependencies and platforms must be described as precisely and a specifically as possible. Parameters values and inputs should be an integral part of the result files. The data behind graphs must be published. Unit tests are a good way to embed self-diagnostics of reproducibility in the code. Reproducibility is hard, yet tremendously necessary.


\clearpage
\section*{Re-usable (R$^{\mathbf 4}$)}

Making your program re-usable means it can be easily used, and modified, by you and other people, inside and outside your lab. Ensuring your program is re-usable is advantageous for a number of reasons.\\

For you, first. Because the you now and the you in two years are two different persons. Details on how to use the code, its limitations, its quirks, may be present to your mind now, but will probably escape you in six months \parencite{Donoho:2009}. Here, comments and documentation can make a significant difference.\\

Furthermore, a source code reflects the results of all the decisions that were made to create it, but not the reasons behind them. In science, where the method and its justification matters as much as the results, those reasons are precious knowledge. For instance, it is not sufficient to observe that a piece of code is doing correct predictions on a learning task, one is interested by the exact algorithm that is employed, and why. In that context, a comment on how a given parameter was chosen (optimization, experimental data, educated guess, â€¦) is a valuable information.\\

Re-usability of course directly benefits others researchers, from your team and outside of it. The easier it is to use your code, the lower is the threshold for other to study, modify and extend it. Scientists constantly face the constraint of time: if a model is available, documented, and can be installed, run and understood all in a few hours, it will be preferred over another that would require weeks to reach the same stage.\\

Since scientists have a tendency to base their work on results they can verify, a reproducible and re-usable code offers a platform both verified and easy-to-use, fostering the development of derivative work by other researchers. Those derivative works contribute to the impact of your original contribution, and generate citations. 


% * <fabien.benureau+overleaf@gmail.com> 2017-06-16T09:22:07.537Z:
% 
% I'd like to refactor this section into:
% 1. Why re-usability is important for you (you will be stranger to yourself in six month)
% 2. Why it is important for others (1. they will be more likely to cite your work, 2. long-term reproducibility)
% 3. Tips and general guidelines on how to do it.
% 
% ^.


Making your program re-usable means it can be used by people outside your lab,
using possibly different environment, different parameters or different data.
If people start using your program, they will most likely report bugs or malfunctions they encountered while using your program.
If you're lucky enough, they will even propose you bugfixes, hence improving the overall quality and correctness of your software.
In other words, if your program is re-usable and is re-used, it is alive.
As such, and as for any other living being, it will evolve and mutate to adapt to its environment.
This process will ensure long-term reproducibility to the extent people continue using and maintaining the program.\\

However, making your program to be re-usable by others requires a little work. If it is natural, when one writes a program for oneself, to make implicit choices that suit one needs, such implicit choices won't resist the will of a user community. For example, having magic numbers or ill-defined constants in your program might be cumbersome for others. Similarly, it might be acceptable to not have a proper documentation when you're the only user (actually it might not be acceptable because the you now and the you in six months are two different persons), but other users want to know precisely how to use this or that function and what it does exactly.\\


If we look back at our random walker, we can list all the implicit choices that have been made:
\begin{itemize}
\item Step number has been hard-coded (magic number 10)
\item Step size has been fixed to 1
\item Initial position has been set to 0
\item No possibility of using a user-specified random seed
\item Only one run can be started at once
\item Walker walk along a single dimension
\item ...
\end{itemize}
However, we won't try to implement all these features at once, but only giving the possibility for a user to implement them if needed.


\begin{code}{\textbf{\textsc{Listing 5:}} Random walk (R$^4$)}
# Copyright (c) 2017 Nicolas P. Rougier and Fabien C.Y. Benureau
# Release under the BSD 2-clause license
# Tested with Python 3.6 / Numpy 1.12.0 / macOS 10.12.4 / 64 bits architecture
import random
import numpy as np

def walk(rng, n):
    """ Random walk for n steps """

    steps = 2*(rng.uniform(-1,+1,n) > 0) - 1
    return steps.cumsum().tolist()

def rng(seed):
    """ Return a random number generator initialized with seed """ 
    
    rng = np.random.RandomState()
    rng.seed(seed)
    return rng

def test():
    """ Unit tests """

    return walk(rng(seed=1), 10) == [-1, 0, 1, 0, -1, -2, -1, 0, -1, -2]

if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser("Random walk")
    parser.add_argument('--seed', type=int, default=1,
                        help='Seed for random number generator ')
    parser.add_argument('n', type=int, default=10,
                        help='Number of step(s) to walk')
    args = parser.parse_args()

    # Random walk for n steps
    x = walk(rng(args.seed), args.n)

    # Display & save results
    print("Seed:", args.seed)
    print("Number of steps:", args.n)
    print("Result:",  x)
    with open("results-R5-%d.txt" % seed, "w") as file:
        file.write("Version: R5")
        file.write("Seed: %d" % args.seed)
        file.write("Steps number: %d" % args.n)
        file.write("Output: %s" % str(x))
\end{code}


% \clearpage
\section*{Replicable (R$^{\mathbf 5}$)}

Having made a software re-usable by others should offer a decent protection against bugs.
Unfortunately, this is not always the case and some recent cases have demonstrated the tremendous impact a bug can have in Science \citep{Eklund:2016} or in our every-day life \citep{Durumeric:2014}.
This is why, as explained by Peng et al. \cite{Peng:2006}, {\em the replication of important findings by multiple independent investigators is fundamental to the accumulation of scientific evidence}.\\
% Replicability is the assumption that any article that does not provide the code source makes: that the description it provides of the algorithms is sufficiently precise and complete to re-obtain the results it presents.

In our case, the replication of the random walk brings some unexpected discrepancies between the original Python version and the replicated Numpy version.
The reason is to be found in a differences in the respective implementations of the RNG between Python and Numpy.
As explained in the \href{https://docs.python.org/3.6/library/random.html}{Python documentation}, {\em Python uses the Mersenne Twister as the core generator. It produces 53-bit precision floats
and has a period of 2**19937-1.
The underlying implementation in C is both fast and threadsafe.}
Numpy RNG is also based on the Mersenne Twister generator but there are differences in the way seed is interpreted when initializing the generator.
Fortunately, Both Numpy and Python offer access to the internal state and we can {\em fix} it to make the behavior of one RNG to match the other RNG.
Here, we want to make the Numpy RNG to match the Python RNG behavior.\\

Without such explicit replication effort, it would be very hard to discover this faulty behavior and this why the ReScience journal has been created.
This journal targets computational research and encourages the explicit replication of already published research, promoting new and open-source implementations in order to ensure that the original research is reproducible.

\begin{code}{\textbf{\textsc{Listing 6:}} Random walk (R$^5$)}
# Copyright (c) 2017 Nicolas P. Rougier and Fabien C.Y. Benureau
# Release under the BSD 2-clause license
# Tested with Python 3.6 / Numpy 1.12.0 / macOS 10.12.4 / 64 bits architecture
import random
import numpy as np

def walk(rng, n):
    """ Random walk for n steps """

    steps = 2*(rng.uniform(-1,+1,n) > 0) - 1
    return steps.cumsum().tolist()

def rng(seed):
    """ Return a random number generator initialized with seed """ 
    
    rng = random.Random()
    rng.seed(seed)
    _, keys, _ = rng.getstate()
    rng = np.random.RandomState()
    state = rng.get_state()
    rng.set_state((state[0], keys[:-1], state[2], state[3], state[4]))
    return rng

if __name__ == '__main__':
    # Unit test
    assert walk(rng(seed=1), 10) == [-1, 0, 1, 0, -1, -2, -1, 0, -1, -2]

    # Random walk for 10 steps
    seed = 1
    x = walk(rng(seed=2), 10)

    # Display & save results
    print(x)
    with open("results-R4-%d.txt" % seed, "w") as file:
        file.write(str(x))
\end{code}



% \clearpage
\section*{Conclusion}

Through a very simple random walk example using Python, we've seen a variety of pitfalls of writing scientific code. The R$^0$ form, while easy to write and simple to understand is not a good scientific code while the full-fledge R$^{4}$ form is much preferred but requires a fair amount of work that might be too much of a burden in some circumstances. $R^3$ form seems to be an acceptable compromise and should be accepted as the minimum scientific standard. This means this should be actually checked by reviewers and publishers when code is part of a work worth to be published. But it's hardly the case today.\\

The choice of the Python programming language to illustrate our point was not arbitrary. In less than a decade, Python has become one the few programming language of Science (together with R and Matlab). It is now used by a huge and growing number of researchers from different domains, having various experience and background in computer science. The readability and ease of use of Python make it a premium choice for new comers. But this is precisely this apparent simplicity that might hinder correct implementation. Inexperienced people may think any working code is a scientific code, but, as we've explained, this is simply not true.


% Writing reproducible software requires both knowledge and experience. For the former, there are plenty of resources online or onsite.

% underscore that compared to psy/bio/etc. the replication issues of CS are easily (reasonably) managed: good solutions that (mostly) works exist right now.

\renewcommand*{\bibfont}{\small}
\printbibliography[title=References]

\newpage 
\appendix

\section{Appendix: Recording Execution Environments}
\label{appendix:executionenvs}
Talk about provenance-tracking and its problems (endianess, "as well as any defaults you may have set that might have an impact.")

Talk about saving VMs, Docker images and its problems.

\end{document}

