% !TeX program = pdflatex
\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}

% Wider margins (compared to default)
\usepackage[margin=2.5cm]{geometry}

% English support (typography and hyphenation)
\usepackage[american]{babel}
\usepackage{csquotes}

% Unicode encoding
\usepackage[utf8]{inputenc}

% Better default font (Libertine and Inconsolata)
\usepackage[ttscale=.875]{libertine}
\usepackage[scaled=0.96]{zi4}

%\usepackage{mathspec}
%\setmainfont{Minion Pro Cond}
%\setmathsfont(Digits,Greek,Latin)[Numbers={Proportional}]{Minion Pro}
%\setmathrm{Minion Pro}
%\setsansfont{MyriadPro-Cond}


% Biber
\usepackage[backend=biber,
            style=apa,
            maxcitenames=3,
            maxbibnames=99,
            apamaxprtauth=99,
            natbib=true]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\bibliography{draft.bib}

% Graphics
\usepackage{graphicx}


% Code Listing
\usepackage{listings}
\usepackage[framemethod=tikz]{mdframed}
\makeatletter
\def\mdf@@codeheading{Code Listings}
\define@key{mdf}{title}{\def\mdf@@codeheading{#1}}
\mdfdefinestyle{lstlisting}{%
  backgroundcolor=black!2.5,
  innertopmargin=2pt,
  middlelinewidth=0.75pt,
  outerlinewidth=9pt,
  outerlinecolor=white,
  innerleftmargin=10pt,
  innerrightmargin=10pt,
  leftmargin=0pt,
  rightmargin=0pt,
  rightline=false,
  leftline=false,
  bottomline=false,
  skipabove=\topskip,
  skipbelow=\topskip,
  roundcorner=0pt,
  singleextra={
    \node[text=black, % fill=white, draw,
          anchor=south west, yshift=-0.25pt, xshift=5pt,
          font=\footnotesize] at (O|-P) {\csname mdf@@codeheading\endcsname};},
  firstextra={
    \node[text=black, % fill=white, draw,
          anchor=south west, yshift=-0.5pt, xshift=5pt,
          font=\footnotesize] at (O|-P) {\csname mdf@@codeheading\endcsname};}
}

\lstset{ %
  basicstyle=\small\tt\linespread{0.75},
  language=Python,
  commentstyle=\color{gray},
  columns=fullflexible,
}

\lstnewenvironment{code}[2][]{%
  \lstset{#1}%
  \mdframed[style=lstlisting,title={#2}]%
}{\endmdframed}


%\usepackage{listings}
%\surroundwithmdframed[
%%  hidealllines=true,
%  linewidth=0.25pt,
%  linecolor=black!50,
%  backgroundcolor=black!4,
%  innerleftmargin=8pt,
%  innertopmargin=3pt,
%  innerbottommargin=3pt]{lstlisting}
%\lstset{ %
%  basicstyle=\small\tt\linespread{0.75},
%  language=Python,
%  commentstyle=\color{gray},
%  columns=fullflexible,
%}


% Hyperref
\usepackage{xcolor}
\definecolor{blendedblue}{rgb}{0.2, 0.2, 0.6}
\definecolor{blendedred}{rgb}{0.8, 0.2, 0.2}
\usepackage[bookmarks=true,
            breaklinks=true,
            pdfborder={0 0 0},
            citecolor=blendedblue,
            colorlinks=true,
            linkcolor=blendedblue,
            urlcolor=blendedblue,
            citecolor=blendedblue,
            linktocpage=false,
            hyperindex=true,
            linkbordercolor=white]{hyperref}
\usepackage{hyperref}
\hypersetup{colorlinks=true}


\title{Re-run, Repeat, Reproduce, Reuse, Replicate:\\Transforming Code into Scientific Contributions}
% \title{Run Python, run!}
% \title{The R Quintuplet (R$^5$)}
% \author{Nicolas P. Rougier and Fabien C. Y. Benureau}
\author{
  \textbf{Fabien C. Y. Benureau}$^{1,2,3,*}$,
  \textbf{Nicolas P. Rougier}$^{1,2,3}$\\ \begin{minipage}{\textwidth}
    \begin{center}
      \vspace{2mm}
      $^{1}$INRIA Bordeaux Sud-Ouest Talence, France $^{2}$ Institut des
      Maladies Neurodégénératives, Université de Bordeaux, CNRS UMR 5293,
      Bordeaux, France $^{3}$ LaBRI, Université de Bordeaux, Bordeaux INP, CNRS
      UMR 5800, Talence, France\\
      \vspace{2mm}
      $^{*}$Corresponding author:
      \href{mailto:fabien.benureau@gmail.com}{fabien.benureau@gmail.com}
    \end{center}
  \end{minipage}
}
\date{}

\begin{document}
\maketitle
% -------------------------
\section*{Introduction (R$^{\mathbf 0}$)}

Replicability is a cornerstone of science.  If an experimental result cannot be
re-obtained by an independent party, it merely becomes, at best, an observation
that may inspire future research (\cite{Mesirov:2010,osc:2015}). Replication
issues have received increased attention in recent years, with a particular focus on medicine and psychology (\cite{Iqbal:2016}).  One could think
that computer science would mostly be shielded from such issues, since a
computer program intrinsically describes precisely what it does and is easily
disseminated to other researchers without alteration.

But precisely because it is easy to believe that if a program runs once and
gives the expected results it will do so forever, crucial steps to transform
working code into meaningful scientific contributions are rarely undertaken
\citep{Sandve:2013,Schwab:2000}. Computer science is plagued by replication
problems, in part, precisely because it seems impervious to them. A program can
fail as a scientific contribution in many different ways for many different
reasons. Borrowing the terms coined by \citeauthor{Goble:2016}
\citep{Goble:2016}, for a program to contribute to science, it should be
re-runnable (R$^1$), repeatable (R$^2$), reproducible (R$^3$),
reusable (R$^4$) and replicable (R$^5$). Let us illustrate this with a small
example, a random walk written in Python:\\

\noindent \begin{minipage}[c]{\linewidth}
\begin{code}{\textbf{\textsc{Listing 1:}} Random walk (R$^0$)}
import random

x = 0
for i in xrange(10):
    step = random.choice([-1,+1])
    x += step
    print x,
\end{code}
\end{minipage}

Executed, this program would display:
\begin{code}{Output}
-1 0 -1 -2 -1 -2 -1 -2 -3 -4 # with the steps being -1, +1, -1, -1, +1, -1, +1, -1, -1,  -1
\end{code}

What could go wrong with such a simple program?\\
\vfill
Well...
\vfill


% -------------------------
\clearpage
\section*{Re-runnable (R$^{\mathbf 1}$)}

Have you ever tried to re-run a program you wrote some years ago? Frustratingly, it can often be surprisingly hard. Part of the problem is that technology is evolving at a fast pace and you cannot know in advance how the system, the software and the libraries your program depends on will evolve. Since you wrote the code, you may have reinstalled or upgraded  your operating system. The compiler, interpreter or set of libraries installed may have been replaced with newer versions. You may find yourself battling with arcane issues of library compatibility---thoroughly orthogonal to your immediate research  goals---to execute again a code \emph{that worked perfectly before}. To be clear, it is impossible to write future-proof code, and the best efforts can be stymied by the smallest change in one of the dependencies. At the same time, modernizing an unmaintained ten-year-old code can reveal itself to be an arduous and expensive undertaking, and precarious as that, since each change risks affecting the semantics of the program. Rather trying to predict the future or painstakingly dusting off old code, an often more straightforward and less risky solution is to recreate the old execution environment\footnote{To be clear, and although virtual machines are often a great help here, this is not always possible. It is, however, \emph{always} more difficult when the original execution environment is not documented.}. For this to happen however, the dependencies in terms of systems, software and libraries must be made clear enough.\\

A \emph{re-runnable} code is one that can be run again when needed, and in particular more than the one time that was needed to produce the results. It is important to notice that the re-runnability of a code is not an intrinsic property. Rather, it depends on the context, and re-runnability becomes increasingly difficult as the code ages. Therefore, to be re-runnable in other researchers' computers and through time, a re-runnable code should describe---with enough details to be recreated---an execution environment in which it is executable. As shown by \citep{Collberg:2016}, this is far from being either obvious or easy.\\

\noindent \begin{minipage}[c]{\linewidth}
\begin{code}{\textbf{\textsc{Listing 2:}} Re-runnable random walk (R$^1$)}
# Tested with Python 3
import random

walk, total = [], 0
for i in range(10):
    step = random.choice([-1,+1])
    total += step
    walk.append(total)

print(walk)
\end{code}
\end{minipage}\\

In our case, the R$^0$ version of our tiny walker seems to imply that any version of Python would be fine. This not the case: it uses the print {\em instruction} and the {\tt xrange} operator, both specific to Python 2. The print {\em instruction}, available in Python 2 (a version still widely used; support is scheduled to stop in 2020), has been deprecated in Python 3 (first released in 2008, almost a decade ago) in favor or a  print {\em function}, while the {\tt xrange} operator has been replaced by the {\tt range} operator in Python 3. In order to try to future-proof the code a bit, we might as well target Python 3, as is done in the R$^1$ version. Incidentally, it remains compatible with Python 2. But whichever version is chosen, the crucial step here is to document it.


% \clearpage
\section*{Repeatable (R$^{\mathbf 2}$)}

The code is running and producing the expected results. The next step is to make sure that you can produce the same output over successive runs of your program. In other words, the next step is to make your program deterministic, producing {\em repeatable} output. Such repeatability is useful. If a run of the program produces a particularly puzzling result, it allows you to scrutinize any step of the execution of the program by re-running it again with extraneous prints, or inside a debugger. It is also the easiest way to prove that the program did indeed produce the published results. Such repeatability is not always possible or easy \citep{Diethelm:2012, Courtes:2015}. But for sequential programs not depending on analog inputs, it often comes down to controlling the initialization of the pseudo-random number generators (RNG).\\

For our program, that means setting the seed of the {\tt random} module. We may also want to save the output of the program to a file, so that we can easily verify that consecutive runs do produce the same output: eyeballing differences is unreliable and  time-consuming, and therefore won't be done systematically.\\

\noindent \begin{minipage}[c]{\linewidth}
\begin{code}{\textbf{\textsc{Listing 3:}} Re-runnable, repeatable random walk (R$^2$)}
# Tested with Python 3
import random

random.seed(0) # RNG initialization

x =  0
path = []
for i in range(10):
    step = random.choice([-1,+1])
    x += step
    path.append(x)

print(path)
# Saving output to disk
with open("results-R2.txt", "w") as fd:
    fd.write(str(path))

\end{code}
\end{minipage}\\

Setting seeds is should be done carefully. Using 439 as a seed in the previous program would result in ten consecutive +1 steps\footnote{With CPython 3.3-3.6. See the next section for details.}, which---although a perfectly valid random walk---lend itself to a gross misinterpretation of the overall dynamic of the algorithm. Verifying that the qualitative aspects of the results and the conclusions that are made are not tied to a specific initialization of the pseudo-random generator is an integral part of any scientific undertaking in computational science;
this is usually done by repeating the simulations multiple times with different seeds.
%FIXME: is `simulations` pertinent here?

% For example, you can play with some parameters and
% find a specific set that produce interesting results that you save to a file.
% In the meantime, you continue playing with parameters in order get new results.
% But what if you want to come back to this one specific result you just saved?
% Did you save as well the set of parameters tied to this specific results?
% This kind of situation happens quite regularly in the scientific literature \cite{Claerbout:2000}
% such that in some case, it is virtually impossible
% to find the set of parameters that has been used to produce this or that figure.
% If you're note careful enough, this will happen to you as well.\\


%\clearpage
\section*{Reproducible (R$^{\mathbf 3}$)}

% Reproducible definition + R^2 not reproducible
The R$^2$ code seems fine enough, but it hides several problems that come to light when trying to {\em reproduce} results. A result is said to be \emph{reproducible} if another researcher can take the original code and input data, execute it, and re-obtain the same result \parencite{Peng:2006}. As explained by \citeauthor{Donoho:2009} \parencite{Donoho:2009}, scientific practice must expect that {\em errors are ubiquitous}, and therefore be robust to them. Ensuring reproducibility is an fundamental step toward this: it provides other researchers the means to verify that your code does indeed produce the published results, and to scrutinize the procedures it used to produce them. As demonstrated by \citeauthor{Mesnard:2016} \citep{Mesnard:2016}, reproducibility is hard.\\

For instance, the R$^2$ program will not produce the same results all the time. It will, because it is repeatable, produce the same results over repeated executions. But it will not necessarily do so over different execution environments. The cause is to be found in a change that occurred in the pseudo-random number generator between Python 3.2 and Python 3.3. Executed with Python 2.7 to 3.2, the code will produce the sequence 1, 2, 1, 2, 3, 4, 5, 6, 7, 6. But with Python 3.3 to 3.6, it will produce 1, 2, 1, 0, 1, 0, 1, 0, -1, 0. With future version of the language, it may change still.\\

% precise execution environment recording
Because any dependency of a program---to the most basic one, the language itself---can change its behavior from one version to another, executability (R$^1$) and determinism (R$^2$) are necessary but not sufficient for reproducibility. The exact execution environment used to produce the results must also be specified---rather than the broadest set of environments where the code can be effectively run. In other words, assertions such as "the results were obtained with CPython 3.6.1" are more valuable, in a scientific context, than "the program works with Python 3.x and above". With the increasing complexity of computational stacks, retrieving and deciding what is pertinent (CPU architecture? operating system version? endianness?) might be non-trivial. A good rule of thumb is to include more information than necessary rather than not enough, and some rather than none.\\

% if code changes, parameters are lost
Recording the execution environment is not enough though. The R$^2$ program uses a random seed but does not keep a trace of it except in the code. Should the code change after the production of the results, someone provided with the last version of the code will not be able to know which seed was used to produce the results, and would need to iterate through all possible random seeds, an impossible task in practice\footnote{Here, with $2^{10}$ possibilities for a 10-step random walk, a seed matching the results could obviously be found. For instance, seed 1123581321 matches the results of R$^0$ with Python 2.7. This is not possible for a 100-step walk.}.\\

% 1. parameters and 2. code version with results
This is why results files should come alongside their context, i.e. an exhaustive list of the parameters used as well as a precise description of the execution environment, as the R$^3$ code does. The code itself is part of that context: the version of the code must be recorded as part of the provenance data. It is common for different results or different figures to have been generated by different versions of the code. Ideally, all results should originate from the same (and last) version of the code. But for long or expensive computations, this may not be feasible. In that case, the result files should contain the version of the code that was used to produce it. This information can be obtained from the version control software. This also allows, if some errors are found and corrected after some results have been obtained, to identify which ones should be recomputed. In R$^3$, the code records the last git commit hash, and whether the repository holds uncommitted changes when the computation starts.\\

% scripts to generates figures & data behind graphs
Published results should come from version of the code where every file has been committed. This includes pre-processing, post-processing and plotting code. Plotting code may seem mundane, but it is as vulnerable as any other piece of the code to bugs and errors. With a code that is re-runnable, repeatable and contains plotting scripts, a user can quickly reproduce published figures. When it comes to checking that the reproduced data match the one published in the article, however, figures can reveal themselves to be imprecise and cumbersome, and sometimes plain unusable. Ideally, published figures should be accompanied by their underlying data (coordinates of the plotted points) in the supplementary data to allow straightforward numeric comparisons.\\

% unit test for R^3
Another good practice is to make the code self-verifiable. In R$^3$, a short unit test is provided, that allows the code to verify its own reproducibility. Should this test fail, then there is little hope of reproducing the results. Of course, passing the test does not guarantee anything.\\

% reproducibility implies availability
It should be obvious by now that \emph{reproducibility implies availability}. As shown in \citep{Collberg:2016}, code is often unavailable, or only available upon request. While the latter may seem sufficient, changes in email address, changes in career, retirement, a busy inbox or poor archiving practices can make a code just as unreachable. Code \emph{and} input data \emph{and} result data should be available with the published article, as supplementary data, or through a DOI link to a scientific repository such as Figshare or Zenodo\footnote{Online code repositories such as GitHub \emph{are not} scientific repositories, and may disappear, change name or change their access policy at any moment. Direct links to them are not perpetual, and, when used, they should always be supplemented by a DOI link to a scientific archive.}.\\

\noindent \begin{minipage}[c]{\linewidth}
\begin{code}{\textbf{\textsc{Listing 4:}} Re-runnable, repeatable, reproducible random walk (R$^3$)}
# Copyright (c) 2017 Nicolas P. Rougier and Fabien C. Y. Benureau
# Release under the BSD 2-clause license
# Tested with CPython 3.6.1 / macOS 10.12.4 / 64 bits architecture
import sys, subprocess, datetime, random

def walk():
    path = []
    x = 0
    for i in range(10):
        if random.uniform(-1,+1) > 0:
            x = x + 1
        else:
            x = x - 1
        path.append(x)
    return path

# If repository is dirty, don't run anything
if subprocess.call(("git", "diff-index", "--quiet", "HEAD")):
    print("Repository is dirty, please commit first")
    sys.exit(1)

# Get git hash if any
revision = subprocess.check_output(("git", "rev-parse", "HEAD"))

# Unit test
random.seed(1)
assert walk() == [-1, 0, 1, 0, -1, -2, -1, 0, -1, -2]

# Random walk for 10 steps
seed = 1
random.seed(seed)
path = walk()

# Display & save results
print(path)
results = { 'data':      path,
            'seed':      seed,
            'timestamp': str(datetime.datetime.utcnow()),
            'revision':  revision,
            'system' :   sys.version }
with open("results-R3.txt", "w") as fd:
    fd.write(str(results))
\end{code}
\end{minipage}\\

% Summary
To recap, reproducibility implies re-runnability and repeatability and availability, yet imposes additional conditions. Dependencies and platforms must be described as precisely and as specifically as possible. Parameters values and inputs should accompany the result files. The data and scripts behind the graphs must be published. Unit tests are a good way to embed self-diagnostics of reproducibility in the code. Reproducibility is hard, yet tremendously necessary.


%\clearpage
\section*{Reusable (R$^{\mathbf 4}$)}

Making your program reusable means it can be easily used, and modified, by you and other people, inside and outside your lab. Ensuring your program is reusable is advantageous for a number of reasons.\\

For you, first. Because the you now and the you in two years are two different persons. Details on how to use the code, its limitations, its quirks, may be present to your mind now, but will probably escape you in six months \parencite{Donoho:2009}. Here, comments and documentation can make a significant difference. Source code reflect the results of the decisions that were made during its creation, but not the reasons behind those decisions. In science, where the method and its justification matter as much as the results, those reasons are precious knowledge. In that context, a comment on how a given parameter was chosen (optimization, experimental data, educated guess), why a library was chosen over another (conceptual or technical reasons?) is valuable information.\\

% Research team typically experience a high turnover of interns, doctoral and post-doctoral students. Can you reproduce the results of member after leave your team?

Reusability of course directly benefits other researchers from your team and outside of it. The easier it is to use your code, the lower the threshold is for other to study, modify and extend it. Scientists constantly face the constraint of time: if a model is available, documented, and can be installed, run and understood all in a few hours, it will be preferred over another that would require weeks to reach the same stage. A reproducible and reusable code offers a platform both \emph{verifiable} and easy-to-use, fostering the development of derivative works by other researchers on solid foundations. Those derivative works contribute to the impact of your original contribution.\\

Having more people examining and using your code also means that potential errors have a higher chance to be caught. If people start using your program, they will most likely report bugs or malfunctions they encounter. If you're lucky enough, they might even propose either bug fixes or improvements, hence improving the overall quality of your software. This process contributes to the long-term reproducibility to the extent people continue to use and maintain the program.\\

Despite all this, reusability is often overlooked, and it is not hard to see why. Scientists are rarely trained in software engineering, and reusability can represent an expensive endeavour if undertaken as an afterthought, for little tangible short-term benefits, for a codebase that might, after all, see only a single use. And, in fact, reusability is not as indispensable a requirement as the other ones presented here. Yet, some simple measures can tremendously increase reusability, and at the same time strengthen reproducibility and re-runnability over the long-term.\\

Avoid hardcoded or magic numbers. Magic numbers are numbers present directly in the source code, that do not have a name and therefore can be difficult to interpret semantically. Hardcoded values are variables that cannot be changed through a function argument or a parameter configuration file. To be modified, they involve editing the code, which is cumbersome and error-prone. In the R$^3$ code, the seed and the number of steps are respectively hardcoded and magic.\\

% However, making your program reusable requires a little work. If it is natural, when one writes a program for oneself, to make implicit choices that suit one needs, such implicit choices won't resist the will of a user community. For example, having magic numbers or ill-defined constants in your program might be cumbersome for others. Similarly, it might be acceptable to not have a proper documentation when you're the only user (actually it might not be acceptable because the you now and the you in six months are two different persons), but other users want to know precisely how to use this or that function and what it does exactly. %FIXME: reword
%
% If we look back at our random walker, we can list all the implicit choices that have been made:
% \begin{itemize}
% \item Step number has been hard-coded (magic number 10)
% \item Step size has been fixed to 1
% \item Initial position has been set to 0
% \item No possibility of using a user-specified random seed
% \item Only one run can be started at once
% \item Walker walk along a single dimension
% \item ...
% \end{itemize}
% Here, we won't try to implement all these features at once, but only giving the possibility for a user to implement them if needed.\\

\noindent \begin{minipage}[c]{\linewidth}
\begin{code}{\textbf{\textsc{Listing 5:}} Re-runnable, repeatable, reproducible, reusable random walk (R$^4$)}
# Copyright (c) 2017 Nicolas P. Rougier and Fabien C. Y. Benureau
# Release under the BSD 2-clause license
# Tested with CPython 3.6.1 / macOS 10.12.4 / 64 bits architecture
import sys, subprocess, datetime, random

def walk(x0=0, step=1, count=10, seed=0):
    """ Random walk
        x0   : initial position (default 0)
        step : step size (default 1)
        count: number of steps (default 10)
        seed : seed for the initialization of the random generator (default 0)
    """
    path, x = [], x0
    random.seed(seed)
    for i in range(count):
        if random.uniform(-1,+1) > 0:
            x = x + step
        else:
            x = x - step
        path.append(x)
    return path


if __name__ == '__main__':
    # If repository is dirty, don't do anything
    if subprocess.call(("git", "diff-index", "--quiet", "HEAD")):
        print("Repository is dirty, please commit first")
        sys.exit(1)

    # Get git hash if any
    revision = subprocess.check_output(("git", "rev-parse", "HEAD"))

    # Unit test checking reproducibility
    assert walk(0, 1, 10, 1) == [-1, 0, 1, 0, -1, -2, -1, 0, -1, -2]

    # Simulation parameters
    parameters = { 'x0':    0,
                   'step':  1,
                   'count': 10,
                   'seed' : 1 }
    path = walk(**parameters)
    results = {'data':       path,
               'parameters': parameters,
               'timestamp':  str(datetime.datetime.utcnow()),
               'revision':   revision,
               'system':     sys.version}

    # Save & display results
    with open("results-R4.txt", "w") as fd:
        fd.write(str(results))

    print(path)
\end{code}
\end{minipage}\\

Similarly, code behavior should not be changed by commenting/uncommenting code \citep{Wilson:2017}. Modification of the behavior of the code, required when different experiments examine slightly different conditions, should always be explicitly set through parameters accessible to the end-user. This improves reproducibility in two ways: it allows those conditions to be explicitly tracked in the result files since they contain the parameters used, and it allows to define separate scripts to run or configuration files to load to produce each of the figures of the published paper. With a documentation explaining which script or configuration file corresponds to which experiment, reproducing the different figures becomes straightforward.\\

Documentation is one of the most potent tools for reusability. A proper documentation on how to install and run the software often makes the difference whether other researchers manage to use the code or not. A comment describing what each function does, however evident, can avoid hours of head-scratching. Great code may need few comments. Scientists, however, are not always brilliant developers. Code quality, in science, averages on mediocre. Of course, bad, complicated code should be rewritten until is simple enough to explain itself. But realistically, this is not always going to be done: there is simply not enough incentive for it. There, a comment that explains the intention and scientific reasons behind a block of code can tremendously useful.\\

Reusability is not a strict requirement for scientific code. But it has many benefits, and a few simple measures can foster it considerably.

% But a few good practices, observed during the development of the code can have significant benefits for you, your team and any other reasearche desirous to use your code.

% \clearpage
\section*{Replicable (R$^{\mathbf 5}$)}

% reusable is not foolproof
Having made a software reusable offers an additional way to find errors, especially if your scientific contribution is popular. Unfortunately, this is not always effective, and some recent cases have dramatically illustrated the tremendous impact a bug can have in science
\citep{Eklund:2016} or in our every-day life \citep{Durumeric:2014}. This is
why, as explained by Peng et al. \cite{Peng:2006}, {\em the replication of
  important findings by multiple independent investigators is fundamental to
  the accumulation of scientific evidence}.\\

% reproducible, a definition
\emph{Replicability} is the implicit
assumption that any article that does not provide the code source makes: that
the description it provides of the algorithms is sufficiently precise and
complete to re-obtain the results it presents. While every published article
should strive for replicability, it is seldom obtained. In fact, absent an explicit effort to make an algorithmic description replicable, there is little probability that it will be.\\

% conceptual description != replicable description
This is because most papers strive to communicate the main ideas behind their contribution is terms as simple and as clear as possible, so that the reader may be able to easily understand them and the results that are presented to him. Trying to ensure replicability in the main text adds a myriad of esoteric details that are not conceptually significant and clutter the explanations. Therefore, unless the writer dedicates an addendum or a section of the supplementary information for technical details specifically aimed at replicability, the information will not be there because there are incentives not to do so.\\

% errors may always remain: reproducibility does not make reproducibility useless.
But even when those details are present, the best efforts may fall short because an oversight, a typo or a difference between what is evident for the writer and for the reader \citep{Mesnard:2016}. Minute changes in the numerical estimation of a common first-order differential equation can have significant impact \citep{Crook:2013}. A reproducible code plays a role that cannot be adequately fulfilled by the conceptual description of it found in the article. Hence, replicability does not negate the necessity of reproducibility. To illustrate this, let us consider what could be the textual description of the random walker (as it would be written in an article describing it):
%
\begin{quotation}
{\em The model uses the Mersene Twister generator initialized with the seed 0. At each iteration, a uniform number between -1 (included) and +1 (excluded) is drawn and the sign of the result is used for generating a positive or negative step.}
\end{quotation}
%
This description, while somewhat precise, forgoes---as it is common---the initialization of the variables (here the starting value of the walk: {\tt 0}), and the technical details about which implementation of the RNG is used.\\

% NumPy and Python: differences in the Mersene Twister
It may look innocuous. After all, the
\href{https://docs.python.org/3.6/library/random.html}{Python documentation},
states that "Python uses the Mersenne Twister as the core generator. It produces
53-bit precision floats and has a period of 2**19937-1". Someone trying to replicate the work however might choose to use the RNG from the \href{http://www.numpy.org/}{NumPy library}.  The NumPy library is extensively used in the science community, and it provides an implementation of the Mersene Twister generator too. Unfortunately, the way the seed is interpreted by the two implementations is different, yielding different random sequences.\\

Here we are able to replicate exactly\footnote{Striving, as we do here, for a perfect quantitative match may seem unnecessary. Yet, in replication projects, quantitative comparisons are an effective way to verify that the behavior has been reproduced. Moreover, they are particularly helpful to track exactly where the code of a tentative replication fails to reproduce the published results.} the behavior of the pure-Python random walker by setting the internal state of the NumPy RNG appropriately, but only because we have access to specific technical details (the use of the {\tt random} module of the standard Python library of CPython 3.6.1), or to the code itself.\\

\noindent \begin{minipage}[c]{\linewidth}
\begin{code}{\textbf{\textsc{Listing 6:}} Replicated random walk (R$^5$)}
# Copyright (c) 2017 Nicolas P. Rougier and Fabien C.Y. Benureau
# Release under the BSD 2-clause license
# Tested with Python 3.6.1 / Numpy 1.12.0 / macOS 10.12.4 / 64 bits architecture
import random
import numpy as np

def _rng(seed):
    """ Return a numpy random number generator initialized with seed
        as it would be with python random generator.
    """
    rng = random.Random()
    rng.seed(seed)
    _, keys, _ = rng.getstate()
    rng = np.random.RandomState()
    state = rng.get_state()
    rng.set_state((state[0], keys[:-1], state[2], state[3], state[4]))
    return rng

def walk(n, seed):
    """ Random walk for n steps """

    rng = _rng(seed)
    steps = 2*(rng.uniform(-1,+1,n) > 0) - 1
    return steps.cumsum().tolist()

if __name__ == '__main__':
    # Unit test
    assert walk(n=10, seed=1) == [-1, 0, 1, 0, -1, -2, -1, 0, -1, -2]

    # Random walk for 10 steps, initialization with seed=1
    seed = 1
    path = walk(n=10, seed=seed)

    # Save & display results
    results = {'data': path, 'seed': seed}
    with open("results-R5.txt", "w") as fd:
        fd.write(str(results))
    print(path)
  
\end{code}
\end{minipage}

But there are still more subtle problems with the description given above.
If we look more closely at it, we can realize
that nothing is said about the specific case of {\tt 0} when generating a step.
Do we have to consider {\tt 0} to be a positive or a negative step? Without
further information and without the original code, it is up to the reader to
decide. Likewise, the description is ambiguous regarding the first element of the walk. Is the initialization value included (it was not in our codes so far)? This slight difference affects the statistics of short runs.\\

All these ambiguities in the description of an algorithm pile up; some are inconsequential (the 0 case has null probability), but some may affect the results in important ways. They are mostly inconspicuous to the reader and oftentimes, to the writer as well. In fact,  there is no better way to ferret out the ambiguities, big and small, of an article than to replicate it. This is one of the reasons why the ReScience journal \citep{Rougier:2017} has been created. This journal {\em targets  computational research and encourages the explicit replication of already published research, promoting new and open-source implementations in order to ensure that the original research is reproducible}.\\

% \clearpage
\section*{Conclusion}

Throughout the evolution of a small random walk example implemented in Python, we illustrated some of the issue that may plague code that work and produce correct results, and reduce its value as an element of scientific knowledge.
Further, we articulated characteristics that a code should possess to be a useful part of a scientific publication: it should be rerunnable, repeatable, reproducible, reusable and replicable.\\

Running old code on tomorrow’s computer and software stacks may not be possible. But recreating the old code’s execution environment may be: to ensure that the long-term rerunnability of a code, its execution environment must be documented. For our example, a single comment went a long way to go from the R$^0$ to the R$^1$ (rerunnable) code.\\

Science is built on verifying the results of others. This is harder to do if each execution of the code produce a different result. While for complex parallel workflow this may not be possible, in all instances where it is feasible the code should be repeatable. This allows future researchers to examine exactly how a specific result was produced. Most of the time, what is needed is to set or record the initial state of the pseudo-random number generator, as what done in the R$^2$ (repeatable) version.\\

Even more care is needed to make a code reproducible. The exact execution environment, code and parameters used must be recorded and embedded in the results files. The R$^3$ (reproducible) version implements provenance methods to that end. Furthermore, the code must be made available as supplementary data with the whole computational workflow, from preprocessing steps to plotting scripts.\\

Making code reusable is a stretch goal that can yield tremendous benefits for you, your team and other researchers. Taken into account during development rather than as an afterthought, simple measures can avoid hours of head-scratching for others, and for yourself—in a few years. Documentation is paramount here, even a single comment per function, as was done in the R$^4$ (reusable) version.\\

Finally, there is the belief that an article should suffice itself: the description of the algorithms present in the paper should suffice to re-obtain (to replicate) the published results. For well-written papers that precisely dissociate conceptually significant details from irrelevant implementation details, that may be. But science should not assume the best of cases. Science assumes that errors are ubiquitous. Every paper is a mistake or a forgotten parameter away from irreproducibility. Replication efforts use the paper first, and then the reproducible code that comes along with it whenever the paper falls short of being precise enough to be reimplemented.\\

In conclusion, the R$^3$ (reproducible) form should be accepted as the minimum
scientific standard \citep{Wilson:2017}. This means this should be actually
checked by reviewers and publishers when code is part of a work worth being
published. But it's hardly the case today.\\

% F: I am not sure that this is really that important a point for the conclusion.
% The choice of the Python programming language to illustrate our point was not arbitrary. In less than a decade, Python has become one the few programming language of Science (together with R and Matlab). It is now used by a huge and growing number of researchers from different domains, having various experience and background in computer science. The readability and ease of use of Python make it a premium choice for new comers. But this is precisely this apparent simplicity that might hinder correct implementation. Inexperienced people may think any working code is a scientific code, but, as we've explained, this is simply not true.

Compared to psychology or biology, the replication issues of computational works have reasonable and efficient solutions. But making sure that these solutions are adopted will not be solved by articles such as this one. Just like in psychology and biology, we have to modify the incentive of the researchers, by adopting exigences, enforced domain-wide, on what constitutes an acceptable scientific computational work.

% Writing reproducible software requires both knowledge and experience. For the former, there are plenty of resources online or onsite.

\renewcommand*{\bibfont}{\small}
\printbibliography[title=References]

% \newpage
% \appendix

% \section{Appendix: Recording Execution Environments}
% \label{appendix:executionenvs}
% Talk about provenance-tracking and its problems (endianess, "as well as any defaults you may have set that might have an impact.")
% Talk about saving VMs, Docker images and its problems.

\end{document}
